{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "60de9724-1060-4d77-9ad2-8a45d17b2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, actions, learning_rate=0.1, gamma=0.9, epsilon=0.9):\n",
    "        self.q_table = defaultdict(lambda: np.zeros(len(actions)))\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_key = tuple(state)\n",
    "        if np.random.uniform() < self.epsilon:    # ε-贪婪策略\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state_key])\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state_key = tuple(state)\n",
    "        next_key = tuple(next_state)\n",
    "        q_predict = self.q_table[state_key][action]\n",
    "        q_target = reward + self.gamma * np.max(self.q_table[next_key])\n",
    "        self.q_table[state_key][action] += self.lr * (q_target - q_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "f51db435-353a-4f23-b8f0-33a724ff4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class Maze(tk.Tk):\n",
    "    UNIT = 40  # 像素单位\n",
    "    MAZE_H = 6  # 网格高度\n",
    "    MAZE_W = 6  # 网格宽度\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title('Maze')\n",
    "        self.geometry(f'{self.MAZE_H * self.UNIT}x{self.MAZE_W * self.UNIT}')\n",
    "        self._build_maze()\n",
    "        self.bind_keys()  # 绑定键盘事件\n",
    "\n",
    "    def _draw_rect(self, x, y, color):\n",
    "        \"\"\"绘制方块并返回对象ID\"\"\"\n",
    "        center = self.UNIT / 2\n",
    "        offset = center - 5\n",
    "        x_center = x * self.UNIT + center\n",
    "        y_center = y * self.UNIT + center\n",
    "        return self.canvas.create_rectangle(\n",
    "            x_center - offset, y_center - offset,\n",
    "            x_center + offset, y_center + offset,\n",
    "            fill=color, outline=''\n",
    "        )\n",
    "\n",
    "    def _build_maze(self):\n",
    "        \"\"\"初始化迷宫布局\"\"\"\n",
    "        # 创建画布\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                               height=self.MAZE_H * self.UNIT,\n",
    "                               width=self.MAZE_W * self.UNIT)\n",
    "    \n",
    "        # 绘制网格线\n",
    "        for i in range(0, self.MAZE_W * self.UNIT, self.UNIT):\n",
    "            self.canvas.create_line(i, 0, i, self.MAZE_H * self.UNIT)\n",
    "        for i in range(0, self.MAZE_H * self.UNIT, self.UNIT):\n",
    "            self.canvas.create_line(0, i, self.MAZE_W * self.UNIT, i)\n",
    "    \n",
    "        # 生成随机目标点（位于右下角区域）\n",
    "        goal_min_x = self.MAZE_W // 2\n",
    "        goal_min_y = self.MAZE_H // 2\n",
    "        possible_goal = [\n",
    "            (x, y) \n",
    "            for x in range(goal_min_x, self.MAZE_W)\n",
    "            for y in range(goal_min_y, self.MAZE_H)\n",
    "            if (x, y) != (0, 0)  # 排除起点\n",
    "        ]\n",
    "        # 如果右下角没有可用坐标则使用全图\n",
    "        if not possible_goal:\n",
    "            possible_goal = [(x, y) \n",
    "                            for x in range(self.MAZE_W)\n",
    "                            for y in range(self.MAZE_H)\n",
    "                            if (x, y) != (0, 0)]\n",
    "        self.goal_x, self.goal_y = random.choice(possible_goal)\n",
    "        self.goal = self._draw_rect(self.goal_x, self.goal_y, 'yellow')\n",
    "    \n",
    "        # 生成随机陷阱（排除起点和目标点）\n",
    "        all_coords = [(x, y) \n",
    "                     for x in range(self.MAZE_W)\n",
    "                     for y in range(self.MAZE_H)\n",
    "                     if (x, y) not in [(0, 0), (self.goal_x, self.goal_y)]]\n",
    "        hell_coords = random.sample(all_coords, 5)  # 保持原5个陷阱数量\n",
    "    \n",
    "        # 绘制陷阱\n",
    "        self.hells = [self._draw_rect(x, y, 'black') for x, y in hell_coords]\n",
    "    \n",
    "        # 玩家初始位置（保持左上角不变）\n",
    "        self.player = self._draw_rect(0, 0, 'red')\n",
    "        \n",
    "        self.canvas.pack()\n",
    "\n",
    "    def bind_keys(self):\n",
    "        \"\"\"绑定键盘事件\"\"\"\n",
    "        self.bind(\"<KeyPress-Up>\", lambda _: self.move(0))\n",
    "        self.bind(\"<KeyPress-Down>\", lambda _: self.move(1))\n",
    "        self.bind(\"<KeyPress-Right>\", lambda _: self.move(2))\n",
    "        self.bind(\"<KeyPress-Left>\", lambda _: self.move(3))\n",
    "        self.focus_set()  # 确保窗口获得焦点\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\"重写移动方法实现自动控制\"\"\"\n",
    "        # 获取当前坐标\n",
    "        x, y = self.get_grid_position()\n",
    "        \n",
    "        # 计算新坐标\n",
    "        new_x, new_y = x, y\n",
    "        if action == 0 and y > 0: new_y -= 1          # 上\n",
    "        elif action == 1 and y < self.MAZE_H-1: new_y += 1 # 下\n",
    "        elif action == 2 and x < self.MAZE_W-1: new_x += 1 # 右\n",
    "        elif action == 3 and x > 0: new_x -= 1        # 左\n",
    "\n",
    "        # 移动玩家\n",
    "        self.canvas.move(self.player, \n",
    "                        (new_x - x) * self.UNIT,\n",
    "                        (new_y - y) * self.UNIT)\n",
    "        \n",
    "        # 获取奖励和终止标志\n",
    "                \n",
    "        reward, done = self.check_game_state(new_x, new_y)\n",
    "        # 获取奖励和终止标志\n",
    "        is_win = reward <=0\n",
    "        \n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "\n",
    "    \n",
    "    def check_game_state(self, x, y):\n",
    "        \"\"\"重写状态检查不重置界面\"\"\"\n",
    "        if (x, y) == (self.goal_x, self.goal_y):\n",
    "            return 5, True\n",
    "        \n",
    "        for hell in self.hells:\n",
    "            if self.canvas.coords(hell) == self.canvas.coords(self.player):\n",
    "                return -10, True\n",
    "        \n",
    "        return -0.5, False\n",
    "        \n",
    "\n",
    "    def get_grid_position(self):\n",
    "        \"\"\"获取当前网格坐标\"\"\"\n",
    "        coords = self.canvas.coords(self.player)\n",
    "        x = int(coords[0] // self.UNIT)\n",
    "        y = int(coords[1] // self.UNIT)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def show_message(self, text):\n",
    "        \"\"\"显示游戏状态信息\"\"\"\n",
    "        self.canvas.create_text(self.MAZE_W * self.UNIT / 2,\n",
    "                               self.MAZE_H * self.UNIT / 2,\n",
    "                               text=text, font=('Arial', 32), fill='red')\n",
    "        self.update()\n",
    "        time.sleep(1)\n",
    "        self.canvas.delete(\"all\")\n",
    "        self._build_maze()\n",
    "    \n",
    "    def get_state(self):\n",
    "        x, y = self.get_grid_position()\n",
    "        state = np.zeros((self.MAZE_H, self.MAZE_W))\n",
    "        state[y, x] = 1  # 玩家位置\n",
    "        state[self.goal_y, self.goal_x] = 2  # 目标位置\n",
    "        for hell in self.hells:\n",
    "            hell_coords = self.canvas.coords(hell)\n",
    "            hell_x = int(hell_coords[0] // self.UNIT)\n",
    "            hell_y = int(hell_coords[1] // self.UNIT)\n",
    "            state[hell_y, hell_x] = -1  # 陷阱位置\n",
    "        return state.flatten()  # 平铺成一维\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"重置游戏\"\"\"\n",
    "        self._build_maze()\n",
    "        return self.get_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "543616f4-84c7-46dd-bf04-39e252ae4535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:   1 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 6\n",
      "Episode:   2 | Time:  0.94s | Steps:  61 | Result: Lose | Reward:  -10.0 | States: 21\n",
      "Episode:   3 | Time:  0.04s | Steps:   3 | Result: Win | Reward:    4.0 | States: 24\n",
      "Episode:   4 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 31\n",
      "Episode:   5 | Time:  0.02s | Steps:   1 | Result: Win | Reward:    5.0 | States: 33\n",
      "Episode:   6 | Time:  0.23s | Steps:  15 | Result: Lose | Reward:   -2.0 | States: 38\n",
      "Episode:   7 | Time:  0.23s | Steps:  15 | Result: Lose | Reward:   -2.0 | States: 46\n",
      "Episode:   8 | Time:  0.70s | Steps:  45 | Result: Lose | Reward:  -17.0 | States: 63\n",
      "Episode:   9 | Time:  0.12s | Steps:   8 | Result: Win | Reward:    1.5 | States: 66\n",
      "Episode:  10 | Time:  0.26s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 75\n",
      "Episode:  11 | Time:  0.24s | Steps:  16 | Result: Lose | Reward:   -2.5 | States: 87\n",
      "Episode:  12 | Time:  0.27s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 94\n",
      "Episode:  13 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 96\n",
      "Episode:  14 | Time:  0.98s | Steps:  63 | Result: Lose | Reward:  -26.0 | States: 114\n",
      "Episode:  15 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 117\n",
      "Episode:  16 | Time:  0.46s | Steps:  30 | Result: Lose | Reward:   -9.5 | States: 127\n",
      "Episode:  17 | Time:  0.18s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 134\n",
      "Episode:  18 | Time:  0.13s | Steps:   8 | Result: Win | Reward:    1.5 | States: 140\n",
      "Episode:  19 | Time:  0.45s | Steps:  29 | Result: Lose | Reward:   -9.0 | States: 153\n",
      "Episode:  20 | Time:  0.13s | Steps:   8 | Result: Win | Reward:    1.5 | States: 159\n",
      "Episode:  21 | Time:  0.11s | Steps:   7 | Result: Win | Reward:    2.0 | States: 164\n",
      "Episode:  22 | Time:  0.83s | Steps:  54 | Result: Lose | Reward:  -21.5 | States: 182\n",
      "Episode:  23 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 189\n",
      "Episode:  24 | Time:  0.12s | Steps:   8 | Result: Win | Reward:    1.5 | States: 196\n",
      "Episode:  25 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 199\n",
      "Episode:  26 | Time:  0.48s | Steps:  31 | Result: Lose | Reward:  -10.0 | States: 207\n",
      "Episode:  27 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 209\n",
      "Episode:  28 | Time:  0.67s | Steps:  43 | Result: Lose | Reward:  -16.0 | States: 226\n",
      "Episode:  29 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 229\n",
      "Episode:  30 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 235\n",
      "Episode:  31 | Time:  0.02s | Steps:   2 | Result: Win | Reward:    4.5 | States: 237\n",
      "Episode:  32 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 241\n",
      "Episode:  33 | Time:  0.40s | Steps:  25 | Result: Lose | Reward:   -7.0 | States: 250\n",
      "Episode:  34 | Time:  0.68s | Steps:  45 | Result: Lose | Reward:  -17.0 | States: 259\n",
      "Episode:  35 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 262\n",
      "Episode:  36 | Time:  0.27s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 273\n",
      "Episode:  37 | Time:  0.57s | Steps:  37 | Result: Lose | Reward:  -13.0 | States: 286\n",
      "Episode:  38 | Time:  0.02s | Steps:   1 | Result: Win | Reward:    5.0 | States: 288\n",
      "Episode:  39 | Time:  0.02s | Steps:   1 | Result: Win | Reward:    5.0 | States: 290\n",
      "Episode:  40 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 293\n",
      "Episode:  41 | Time:  1.03s | Steps:  66 | Result: Lose | Reward:  -27.5 | States: 308\n",
      "Episode:  42 | Time:  0.01s | Steps:   1 | Result: Win | Reward:    5.0 | States: 310\n",
      "Episode:  43 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 314\n",
      "Episode:  44 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 318\n",
      "Episode:  45 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 320\n",
      "Episode:  46 | Time:  0.12s | Steps:   8 | Result: Win | Reward:    1.5 | States: 322\n",
      "Episode:  47 | Time:  0.20s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 329\n",
      "Episode:  48 | Time:  0.23s | Steps:  15 | Result: Lose | Reward:   -2.0 | States: 338\n",
      "Episode:  49 | Time:  0.61s | Steps:  40 | Result: Win | Reward:    0.5 | States: 354\n",
      "Episode:  50 | Time:  0.27s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 361\n",
      "Episode:  51 | Time:  0.32s | Steps:  21 | Result: Lose | Reward:   -5.0 | States: 370\n",
      "Episode:  52 | Time:  0.30s | Steps:  20 | Result: Lose | Reward:   -4.5 | States: 380\n",
      "Episode:  53 | Time:  0.60s | Steps:  39 | Result: Lose | Reward:  -14.0 | States: 395\n",
      "Episode:  54 | Time:  0.31s | Steps:  20 | Result: Lose | Reward:   -4.5 | States: 402\n",
      "Episode:  55 | Time:  0.04s | Steps:   3 | Result: Win | Reward:    4.0 | States: 405\n",
      "Episode:  56 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 407\n",
      "Episode:  57 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 411\n",
      "Episode:  58 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 418\n",
      "Episode:  59 | Time:  0.32s | Steps:  21 | Result: Lose | Reward:   -5.0 | States: 428\n",
      "Episode:  60 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 431\n",
      "Episode:  61 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 433\n",
      "Episode:  62 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 435\n",
      "Episode:  63 | Time:  0.37s | Steps:  24 | Result: Lose | Reward:   -6.5 | States: 447\n",
      "Episode:  64 | Time:  0.15s | Steps:  10 | Result: Win | Reward:    0.5 | States: 452\n",
      "Episode:  65 | Time:  0.21s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 458\n",
      "Episode:  66 | Time:  0.22s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 465\n",
      "Episode:  67 | Time:  0.18s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 471\n",
      "Episode:  68 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 474\n",
      "Episode:  69 | Time:  0.91s | Steps:  59 | Result: Lose | Reward:   -9.0 | States: 495\n",
      "Episode:  70 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 497\n",
      "Episode:  71 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 502\n",
      "Episode:  72 | Time:  0.50s | Steps:  32 | Result: Lose | Reward:  -10.5 | States: 512\n",
      "Episode:  73 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 517\n",
      "Episode:  74 | Time:  0.22s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 523\n",
      "Episode:  75 | Time:  0.19s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 528\n",
      "Episode:  76 | Time:  0.74s | Steps:  48 | Result: Lose | Reward:  -18.5 | States: 542\n",
      "Episode:  77 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 545\n",
      "Episode:  78 | Time:  0.48s | Steps:  31 | Result: Lose | Reward:  -10.0 | States: 558\n",
      "Episode:  79 | Time:  0.29s | Steps:  19 | Result: Lose | Reward:   -4.0 | States: 567\n",
      "Episode:  80 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 571\n",
      "Episode:  81 | Time:  0.26s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 581\n",
      "Episode:  82 | Time:  0.40s | Steps:  26 | Result: Lose | Reward:   -7.5 | States: 597\n",
      "Episode:  83 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 601\n",
      "Episode:  84 | Time:  0.20s | Steps:  13 | Result: Lose | Reward:   -1.0 | States: 609\n",
      "Episode:  85 | Time:  0.16s | Steps:  10 | Result: Win | Reward:    0.5 | States: 615\n",
      "Episode:  86 | Time:  0.22s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 622\n",
      "Episode:  87 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 625\n",
      "Episode:  88 | Time:  0.01s | Steps:   1 | Result: Win | Reward:    5.0 | States: 627\n",
      "Episode:  89 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 631\n",
      "Episode:  90 | Time:  0.25s | Steps:  16 | Result: Lose | Reward:   -2.5 | States: 644\n",
      "Episode:  91 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 647\n",
      "Episode:  92 | Time:  0.13s | Steps:   9 | Result: Win | Reward:    1.0 | States: 651\n",
      "Episode:  93 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 654\n",
      "Episode:  94 | Time:  0.28s | Steps:  18 | Result: Lose | Reward:   -3.5 | States: 663\n",
      "Episode:  95 | Time:  0.09s | Steps:   7 | Result: Win | Reward:    2.0 | States: 666\n",
      "Episode:  96 | Time:  0.08s | Steps:   6 | Result: Win | Reward:   17.5 | States: 673\n",
      "Episode:  97 | Time:  0.40s | Steps:  25 | Result: Lose | Reward:   -7.0 | States: 684\n",
      "Episode:  98 | Time:  0.60s | Steps:  39 | Result: Lose | Reward:  -14.0 | States: 693\n",
      "Episode:  99 | Time:  0.24s | Steps:  15 | Result: Lose | Reward:   -2.0 | States: 703\n",
      "Episode: 100 | Time:  0.15s | Steps:  10 | Result: Win | Reward:    0.5 | States: 709\n",
      "Episode: 101 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 713\n",
      "Episode: 102 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 716\n",
      "Episode: 103 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 719\n",
      "Episode: 104 | Time:  0.93s | Steps:  60 | Result: Lose | Reward:  -24.5 | States: 735\n",
      "Episode: 105 | Time:  0.04s | Steps:   3 | Result: Win | Reward:    4.0 | States: 737\n",
      "Episode: 106 | Time:  0.19s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 742\n",
      "Episode: 107 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 745\n",
      "Episode: 108 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 748\n",
      "Episode: 109 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 750\n",
      "Episode: 110 | Time:  0.17s | Steps:  11 | Result: Win | Reward:   15.0 | States: 761\n",
      "Episode: 111 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 767\n",
      "Episode: 112 | Time:  0.29s | Steps:  19 | Result: Lose | Reward:   -4.0 | States: 778\n",
      "Episode: 113 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 780\n",
      "Episode: 114 | Time:  0.31s | Steps:  20 | Result: Lose | Reward:   -4.5 | States: 789\n",
      "Episode: 115 | Time:  0.23s | Steps:  15 | Result: Lose | Reward:   -2.0 | States: 795\n",
      "Episode: 116 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 797\n",
      "Episode: 117 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 804\n",
      "Episode: 118 | Time:  0.68s | Steps:  44 | Result: Lose | Reward:  -16.5 | States: 818\n",
      "Episode: 119 | Time:  0.40s | Steps:  26 | Result: Lose | Reward:   -7.5 | States: 827\n",
      "Episode: 120 | Time:  0.62s | Steps:  41 | Result: Lose | Reward:  -15.0 | States: 846\n",
      "Episode: 121 | Time:  1.20s | Steps:  77 | Result: Lose | Reward:  -18.0 | States: 862\n",
      "Episode: 122 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 866\n",
      "Episode: 123 | Time:  1.02s | Steps:  66 | Result: Lose | Reward:  -27.5 | States: 879\n",
      "Episode: 124 | Time:  0.24s | Steps:  16 | Result: Lose | Reward:   -2.5 | States: 886\n",
      "Episode: 125 | Time:  0.25s | Steps:  16 | Result: Lose | Reward:   -2.5 | States: 891\n",
      "Episode: 126 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 897\n",
      "Episode: 127 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 899\n",
      "Episode: 128 | Time:  0.30s | Steps:  19 | Result: Lose | Reward:   -4.0 | States: 902\n",
      "Episode: 129 | Time:  0.04s | Steps:   3 | Result: Win | Reward:    4.0 | States: 904\n",
      "Episode: 130 | Time:  0.21s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 912\n",
      "Episode: 131 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 917\n",
      "Episode: 132 | Time:  0.33s | Steps:  22 | Result: Lose | Reward:   -5.5 | States: 927\n",
      "Episode: 133 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 931\n",
      "Episode: 134 | Time:  0.32s | Steps:  21 | Result: Lose | Reward:   -5.0 | States: 937\n",
      "Episode: 135 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 942\n",
      "Episode: 136 | Time:  0.99s | Steps:  65 | Result: Lose | Reward:  -27.0 | States: 955\n",
      "Episode: 137 | Time:  0.34s | Steps:  22 | Result: Lose | Reward:   -5.5 | States: 967\n",
      "Episode: 138 | Time:  0.11s | Steps:   7 | Result: Win | Reward:    2.0 | States: 972\n",
      "Episode: 139 | Time:  1.29s | Steps:  83 | Result: Lose | Reward:  -36.0 | States: 990\n",
      "Episode: 140 | Time:  0.18s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 994\n",
      "Episode: 141 | Time:  0.02s | Steps:   1 | Result: Win | Reward:    5.0 | States: 996\n",
      "Episode: 142 | Time:  0.34s | Steps:  22 | Result: Lose | Reward:   -5.5 | States: 1002\n",
      "Episode: 143 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 1005\n",
      "Episode: 144 | Time:  0.30s | Steps:  19 | Result: Lose | Reward:   -4.0 | States: 1010\n",
      "Episode: 145 | Time:  0.61s | Steps:  40 | Result: Lose | Reward:  -14.5 | States: 1020\n",
      "Episode: 146 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 1023\n",
      "Episode: 147 | Time:  0.10s | Steps:   6 | Result: Win | Reward:    2.5 | States: 1029\n",
      "Episode: 148 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 1032\n",
      "Episode: 149 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 1036\n",
      "Episode: 150 | Time:  0.18s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 1041\n",
      "Episode: 151 | Time:  0.22s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 1053\n",
      "Episode: 152 | Time:  0.16s | Steps:  10 | Result: Win | Reward:    0.5 | States: 1057\n",
      "Episode: 153 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 1062\n",
      "Episode: 154 | Time:  0.01s | Steps:   1 | Result: Win | Reward:    5.0 | States: 1064\n",
      "Episode: 155 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 1070\n",
      "Episode: 156 | Time:  0.34s | Steps:  22 | Result: Lose | Reward:   -5.5 | States: 1080\n",
      "Episode: 157 | Time:  0.19s | Steps:  13 | Result: Lose | Reward:   -1.0 | States: 1086\n",
      "Episode: 158 | Time:  0.28s | Steps:  18 | Result: Lose | Reward:   -3.5 | States: 1095\n",
      "Episode: 159 | Time:  0.77s | Steps:  50 | Result: Lose | Reward:   -4.5 | States: 1119\n",
      "Episode: 160 | Time:  0.43s | Steps:  28 | Result: Lose | Reward:   -8.5 | States: 1132\n",
      "Episode: 161 | Time:  0.12s | Steps:   8 | Result: Win | Reward:    1.5 | States: 1136\n",
      "Episode: 162 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 1140\n",
      "Episode: 163 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 1143\n",
      "Episode: 164 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 1148\n",
      "Episode: 165 | Time:  0.37s | Steps:  24 | Result: Lose | Reward:   -6.5 | States: 1155\n",
      "Episode: 166 | Time:  0.31s | Steps:  20 | Result: Lose | Reward:   -4.5 | States: 1162\n",
      "Episode: 167 | Time:  0.15s | Steps:  10 | Result: Win | Reward:    0.5 | States: 1168\n",
      "Episode: 168 | Time:  0.56s | Steps:  36 | Result: Lose | Reward:  -12.5 | States: 1181\n",
      "Episode: 169 | Time:  0.09s | Steps:   6 | Result: Win | Reward:    2.5 | States: 1188\n",
      "Episode: 170 | Time:  0.18s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 1195\n",
      "Episode: 171 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 1203\n",
      "Episode: 172 | Time:  0.57s | Steps:  37 | Result: Lose | Reward:  -13.0 | States: 1211\n",
      "Episode: 173 | Time:  0.19s | Steps:  12 | Result: Lose | Reward:   -0.5 | States: 1219\n",
      "Episode: 174 | Time:  0.76s | Steps:  49 | Result: Lose | Reward:  -19.0 | States: 1231\n",
      "Episode: 175 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 1233\n",
      "Episode: 176 | Time:  0.20s | Steps:  13 | Result: Lose | Reward:   -1.0 | States: 1243\n",
      "Episode: 177 | Time:  0.07s | Steps:   5 | Result: Win | Reward:    3.0 | States: 1247\n",
      "Episode: 178 | Time:  0.10s | Steps:   6 | Result: Win | Reward:    2.5 | States: 1253\n",
      "Episode: 179 | Time:  0.14s | Steps:   9 | Result: Win | Reward:    1.0 | States: 1257\n",
      "Episode: 180 | Time:  0.20s | Steps:  13 | Result: Lose | Reward:   -1.0 | States: 1264\n",
      "Episode: 181 | Time:  0.26s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 1273\n",
      "Episode: 182 | Time:  0.26s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 1281\n",
      "Episode: 183 | Time:  0.56s | Steps:  36 | Result: Win | Reward:    2.5 | States: 1298\n",
      "Episode: 184 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 1302\n",
      "Episode: 185 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 1307\n",
      "Episode: 186 | Time:  0.45s | Steps:  29 | Result: Lose | Reward:   -9.0 | States: 1318\n",
      "Episode: 187 | Time:  0.10s | Steps:   7 | Result: Win | Reward:    2.0 | States: 1323\n",
      "Episode: 188 | Time:  0.08s | Steps:   5 | Result: Win | Reward:    3.0 | States: 1327\n",
      "Episode: 189 | Time:  0.22s | Steps:  14 | Result: Lose | Reward:   -1.5 | States: 1331\n",
      "Episode: 190 | Time:  0.02s | Steps:   1 | Result: Win | Reward:    5.0 | States: 1333\n",
      "Episode: 191 | Time:  0.12s | Steps:   8 | Result: Win | Reward:    1.5 | States: 1338\n",
      "Episode: 192 | Time:  0.03s | Steps:   2 | Result: Win | Reward:    4.5 | States: 1341\n",
      "Episode: 193 | Time:  0.17s | Steps:  11 | Result: Lose | Reward:    0.0 | States: 1347\n",
      "Episode: 194 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 1349\n",
      "Episode: 195 | Time:  0.32s | Steps:  21 | Result: Lose | Reward:   -5.0 | States: 1354\n",
      "Episode: 196 | Time:  0.26s | Steps:  17 | Result: Lose | Reward:   -3.0 | States: 1362\n",
      "Episode: 197 | Time:  0.06s | Steps:   4 | Result: Win | Reward:    3.5 | States: 1365\n",
      "Episode: 198 | Time:  0.05s | Steps:   3 | Result: Win | Reward:    4.0 | States: 1368\n",
      "Episode: 199 | Time:  0.33s | Steps:  22 | Result: Lose | Reward:   -5.5 | States: 1376\n",
      "Episode: 200 | Time:  1.09s | Steps:  70 | Result: Lose | Reward:  -29.5 | States: 1398\n",
      "\n",
      "===== Training Summary =====\n",
      "Success Rate: 49.0%\n",
      "Average Time: 0.25s\n",
      "Average Step: 16.16 per each\n",
      "Explored States: 1398\n"
     ]
    }
   ],
   "source": [
    "class AutoMaze(Maze):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.agent = QLearningAgent(actions=[0, 1, 2, 3])\n",
    "        self.training_data = []\n",
    "        self.start_time = None\n",
    "        self.episode_count = 0\n",
    "        self.max_episodes = 200\n",
    "        self.delay = 10  # 可视化延迟（毫秒）\n",
    "        \n",
    "        # 禁用键盘控制\n",
    "        self.unbind(\"<KeyPress>\")\n",
    "        self.start_training()\n",
    "\n",
    "    def unbind(self, sequence):\n",
    "        \"\"\"解除键盘事件绑定\"\"\"\n",
    "        self.bind(sequence, lambda event: None)\n",
    "\n",
    "    def start_training(self):\n",
    "        \"\"\"启动训练流程\"\"\"\n",
    "        self.after(0, self.run_episode)\n",
    "\n",
    "    def run_episode(self):\n",
    "        \"\"\"执行单次训练回合\"\"\"\n",
    "        if self.episode_count >= self.max_episodes:\n",
    "            self.analyze_training_data()\n",
    "            return\n",
    "            \n",
    "        state = self.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        self.start_time = time.time()\n",
    "        self.episode_count += 1\n",
    "        \n",
    "        def step():\n",
    "            nonlocal state, total_reward, done, steps\n",
    "            ##初始分值计算\n",
    "            if steps == 0:\n",
    "                total_reward = 15\n",
    "            action = self.agent.choose_action(state)\n",
    "            next_state, reward, done = self.move(action)\n",
    "            self.agent.learn(state, action, reward, next_state)\n",
    "            result = \"\"\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if not done:\n",
    "                self.after(self.delay, step)\n",
    "            else:\n",
    "                if total_reward > 0:\n",
    "                    result = \"Win\"\n",
    "                else:\n",
    "                    result = \"Lose\"\n",
    "                episode_time = time.time() - self.start_time\n",
    "                self.training_data.append({\n",
    "                    \"episode\": self.episode_count,\n",
    "                    \"time\": episode_time,\n",
    "                    \"steps\": steps,\n",
    "                    \"reward\": total_reward,\n",
    "                    \"result\" : result\n",
    "                })\n",
    "                self.print_progress()\n",
    "                self.after(0, self.run_episode)\n",
    "        \n",
    "        self.after(self.delay, step)\n",
    "\n",
    "\n",
    "\n",
    "    def print_progress(self):\n",
    "        \"\"\"打印训练进度\"\"\"\n",
    "        data = self.training_data[-1]\n",
    "        print(f\"Episode: {data['episode']:3d} | \"\n",
    "              f\"Time: {data['time']:5.2f}s | \"\n",
    "              f\"Steps: {data['steps']:3d} | \" \n",
    "              f\"Result: {data['result']} | \"\n",
    "              f\"Reward: {data['reward']:6.1f} | \"\n",
    "              f\"States: {len(self.agent.q_table)}\")\n",
    "\n",
    "    def analyze_training_data(self):\n",
    "        \"\"\"分析训练结果\"\"\"\n",
    "        print(\"\\n===== Training Summary =====\")\n",
    "        success = sum(1 for d in self.training_data if d[\"reward\"] > 0)\n",
    "        avg_time = np.mean([d[\"time\"] for d in self.training_data])\n",
    "        avg_step = np.mean([d[\"steps\"] for d in self.training_data])\n",
    "        print(f\"Success Rate: {success/len(self.training_data)*100:.1f}%\")\n",
    "        print(f\"Average Time: {avg_time:.2f}s\")\n",
    "        print(f\"Average Step: {avg_step} per each\")\n",
    "        print(f\"Explored States: {len(self.agent.q_table)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = AutoMaze()\n",
    "    env.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
